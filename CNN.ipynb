{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN1-49yj4cDj",
        "colab_type": "text"
      },
      "source": [
        "This is the python code for the CNN in our paper, \"*Convolutional Neural Networks for Direct Detection of Dark Matter*\". \n",
        "\n",
        "The CNN was origionally run using Google Colab GPU and so data was stored in a google drive. \n",
        "\n",
        "This code (and final model, final_model.h5) is for just the \"HitPeak\" images - the \"Hit\" and \"Peak\" images require their own individual models.\n",
        "\n",
        "This file should not be run all at once:\n",
        "*   The hyperparameters first need to be optimised using the 'Train' dataset (1). \n",
        "*   Once these are optimised, the CNN is run again using these hyperparamters on the 'Train' dataset (2) - This saves the model in a h5 file.\n",
        "*   The saved model is run using the 'Test' dataset (3).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjg6UolxSeog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install keras\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install imageio\n",
        "!pip install h5py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsZ-uczaUArI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip drive/My\\ Drive/Colab\\ Notebooks/Train.zip\n",
        "!unzip drive/My\\ Drive/Colab\\ Notebooks/Test.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUP5b4IfVW7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import os.path as path\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "from datetime import datetime\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, precision_recall_curve, auc,recall_score\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "import h5py\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.backend import cast, greater, clip, floatx,epsilon\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYrhHiaGVbEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define image path (e.g.)\n",
        "\n",
        "IMAGE_PATH = '/content/Train'\n",
        "file_paths = glob.glob(path.join(IMAGE_PATH, '*.png'))\n",
        "\n",
        "# Load the images into a single variable and convert to a numpy array\n",
        "images = [imageio.imread(path) for path in file_paths]\n",
        "images = np.asarray(images)\n",
        "\n",
        "# Get image size\n",
        "image_size = np.asarray([images.shape[1], images.shape[2], images.shape[3]])\n",
        "print(image_size)\n",
        "\n",
        "# Scale images so values are between 0 and 1\n",
        "images = images / 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzpWtttMVeO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the labels from the filenames\n",
        "\n",
        "n_images = images.shape[0]\n",
        "labels = np.zeros(n_images)\n",
        "for i in range(n_images):\n",
        "    filename = path.basename(file_paths[i])[0]\n",
        "    if filename[0] == 'W':                          #Every file that begins with W is assigned a 1\n",
        "        labels[i] = 1\n",
        "    else:\n",
        "        labels[i] = 0\n",
        "\n",
        "# Background = 0 = FALSE\n",
        "# WIMPS = 1 = TRUE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOXmwDAh4_q7",
        "colab_type": "text"
      },
      "source": [
        "For gridsearch the data does not need to be split into \"train\" and \"validation\" sets as this is done automatically. \n",
        "\n",
        "This split only needs to be done when running the final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcCJhuAcVgHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into test and training sets\n",
        "\n",
        "TRAIN_TEST_SPLIT = 0.7            \n",
        "\n",
        "# Split at the given index\n",
        "split_index = int(TRAIN_TEST_SPLIT * n_images)\n",
        "shuffled_indices = np.random.permutation(n_images)\n",
        "train_indices = shuffled_indices[0:split_index]\n",
        "test_indices = shuffled_indices[split_index:]\n",
        "\n",
        "# Split the images and the labels\n",
        "x_train = images[train_indices, :, :, :]\n",
        "y_train = labels[train_indices]\n",
        "x_val = images[test_indices, :, :, :]\n",
        "y_val = labels[test_indices]\n",
        "\n",
        "#x_train = images\n",
        "#y_train = labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb8wyjDevr-h",
        "colab_type": "text"
      },
      "source": [
        "(1) Grid search run for testing hyperparameters:\n",
        "*   Convolutional layers - 1, 2, 3\n",
        "*   Epochs - 20, 30, 40, 50, 60\n",
        "*   Batch size - 100, 200, 300\n",
        "*   Learning rate - 0.005, 0.001, 0.01\n",
        "*   Alpha value - 0.001, 0.05, 0.01, 0.1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqDsIBI_nTLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grid search\n",
        "from keras import backend as K\n",
        "np.random.seed(0)\n",
        "\n",
        "# Define recall, precision and f1 functions\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "\n",
        "def create_model(conv_layers, learning_rate, alpha_value):\n",
        "  model = Sequential()\n",
        "  for i in range(conv_layers):\n",
        "    model.add(Conv2D(16, (3,3), strides = (1,1), kernel_regularizer=l2(0.005)))\n",
        "    model.add(LeakyReLU(alpha = alpha_value))\n",
        "    model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1)))\n",
        "    \n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(32, bias_regularizer=l2(0.001), kernel_regularizer=l2(0.001)))\n",
        "  model.add(LeakyReLU(alpha=alpha_value))\n",
        "  \n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(1))\n",
        "  model.add(Activation('sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam(lr=learning_rate), metrics=['acc', f1_m, precision_m, recall_m])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "\n",
        "params_to_test_gen = {'epochs':[20,30,40,50,60], 'batch_size': [100,200,300], 'conv_layers':[1,2,3], 'learning_rate':[0.005,0.001,0.01], 'alpha_value':[0.001, 0.05, 0.01, 0.1]}\n",
        "\n",
        "gsearch_0 = GridSearchCV(estimator = model, param_grid = params_to_test_gen, scoring='recall', n_jobs= None, iid=False, cv=StratifiedKFold(n_splits=3, shuffle = True, random_state = 0), verbose = 5)\n",
        "gsearch_0.fit(x_train, y_train, verbose=1, validation_data=(x_val, y_val))\n",
        "print(\"Best score: \", gsearch_0.best_score_)\n",
        "print(\"Best parameters: \", gsearch_0.best_params_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROSccBtFvgwT",
        "colab_type": "text"
      },
      "source": [
        "(2) Run CNN again using 'optimal hyperparameters' and save model as \"final_model.h5\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB2zinL70q7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "np.random.seed(0)\n",
        "\n",
        "# Define recall, precision and f1 functions\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "  \n",
        "# 1 conv layer / 100 bs / 50 epoch HitPeak images\n",
        "def CNN_model():\n",
        "    shape = (image_size[0], image_size[1], image_size[2])\n",
        "    model = Sequential()\n",
        "    \n",
        "    # 1 conv layer\n",
        "    model.add(Conv2D(16, (3,3), strides = (1,1), input_shape=shape,kernel_regularizer=l2(0.005)))\n",
        "    model.add(keras.layers.LeakyReLU(alpha=0.05))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides = (1,1)))\n",
        "\n",
        "    # 2 conv layers\n",
        "    #model.add(Conv2D(16,(3,3),kernel_regularizer=l2(0.005)))\n",
        "    #model.add(keras.layers.LeakyReLU(alpha=0.05))\n",
        "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # 3 conv layers\n",
        "    #model.add(Conv2D(16,(3,3),kernel_regularizer=l2(0.005)))\n",
        "    #model.add(keras.layers.LeakyReLU(alpha=0.05))\n",
        "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Dropout(0.25)) \n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(32, bias_regularizer=regularizers.l2(0.001),kernel_regularizer=regularizers.l2(0.001)))\n",
        "    model.add(keras.layers.LeakyReLU(alpha=0.05))    \n",
        "    model.add(Dropout(0.5)) \n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                    optimizer='adam',\n",
        "                    metrics=['acc', f1_m, precision_m, recall_m])\n",
        "    return model\n",
        "\n",
        "def run_CNN():\n",
        "    model = CNN_model()\n",
        "    model.fit(x_train, y_train, epochs=50, batch_size=100, verbose=1, validation_data=(x_val, y_val))\n",
        "    model.save('final_model_3.h5')\n",
        "    #model.save_weights('final_weights.h5')\n",
        "    score = model.evaluate(x_val, y_val, verbose=1)\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "    print('Test f1:', score[2])\n",
        "    print('Test precision:', score[3])\n",
        "    print('Test recall:', score[4])\n",
        "\n",
        "run_CNN()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_dD5FYJdpqa",
        "colab_type": "text"
      },
      "source": [
        "(3) Call the saved model (\"final_model.h5\") and run using the test data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpZemf595IoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting/Printing Results\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "np.random.seed(0)\n",
        "\n",
        "# Define recall, precision and f1 functions\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "    \n",
        "IMAGE_PATH = '/content/Test'\n",
        "file_paths = glob.glob(path.join(IMAGE_PATH, '*.png'))\n",
        "\n",
        "# Load the images into a single variable and convert to a numpy array \n",
        "images = [imageio.imread(path) for path in file_paths]\n",
        "images = np.asarray(images)\n",
        "\n",
        "# Get image size \n",
        "image_size = np.asarray([images.shape[1], images.shape[2], images.shape[3]])\n",
        "print(image_size)\n",
        "\n",
        "# Scale images so values are between 0 and 1 \n",
        "images = images / 255\n",
        "\n",
        "n_images = images.shape[0]\n",
        "labels = np.zeros(n_images)\n",
        "for i in range(n_images):\n",
        "    filename = path.basename(file_paths[i])[0]\n",
        "    if filename[0] == 'W':            #Every file that begins with W is assigned a 1\n",
        "        labels[i] = 1\n",
        "    else:\n",
        "        labels[i] = 0\n",
        "\n",
        "x_test = images\n",
        "y_test = labels\n",
        "\n",
        "dependencies = {\n",
        "    'f1_m' : f1_m, \n",
        "    'precision_m' : precision_m,\n",
        "    'recall_m': recall_m,\n",
        "}\n",
        "\n",
        "model = keras.models.load_model('/content/drive/My Drive/Colab Notebooks/final_model.h5', custom_objects=dependencies)\n",
        "\n",
        "# Summarize the model \n",
        "model.summary()\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import auc\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "lr_probs = model.predict_proba(x_test)    # Predict probabilities\n",
        "yhat = model.predict(x_test)      # Predict class values\n",
        "yhat = np.round(yhat)\n",
        "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
        "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
        "print('Logistic: f1=%.3f auc=%.7f' % (lr_f1, lr_auc))   # Summarise scores\n",
        "''' plot the precision-recall curves '''\n",
        "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
        "pyplot.plot(lr_recall, lr_precision, 'r-', label='F1 = %0.2f' %(lr_f1))\n",
        "pyplot.xlabel('Recall')\n",
        "pyplot.ylabel('Precision')\n",
        "pyplot.legend(loc='lower left')\n",
        "plt.title('Precision-Recall (PR) Curve')\n",
        "pyplot.show()\n",
        "\n",
        "# Save precision and recall data to plot PR curve seperately\n",
        "np.savetxt(\"/content/drive/My Drive/Colab Notebooks/precision_HP.txt\",lr_precision)\n",
        "np.savetxt(\"/content/drive/My Drive/Colab Notebooks/recall_HP.txt\",lr_recall)\n",
        "\n",
        "\n",
        "test_predictions = yhat             # Make a prediction on the test set\n",
        "accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "average_precision = average_precision_score(y_test, test_predictions)\n",
        "print(\"Average precision: \" + str(average_precision))\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, threshold = precision_recall_curve(y_test, test_predictions)\n",
        "auc = auc(recall, precision)\n",
        "recall1 = recall_score(y_test, np.round(test_predictions))\n",
        "print(\"recall: \" + str(recall1))\n",
        "print('AUC:' +str(auc))\n",
        "\n",
        "\n",
        "# Report Confusion Matrix\n",
        "y_actu = pd.Series(y_test.ravel(), name='Actual')\n",
        "y_pred = pd.Series(np.round(test_predictions.ravel()), name='Predicted')\n",
        "df_confusion = pd.crosstab(y_actu, y_pred)\n",
        "print(df_confusion)\n",
        "\n",
        "# Plot confusion matrix\n",
        "def plot_confusion_matrix(df_confusion, cmap='YlGn'):\n",
        "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(0,len(df_confusion.columns))\n",
        "    plt.xticks(tick_marks, df_confusion.columns)\n",
        "    plt.yticks(tick_marks, df_confusion.index)\n",
        "    plt.ylabel(df_confusion.index.name)\n",
        "    plt.xlabel(df_confusion.columns.name)\n",
        "    for i in range(len(df_confusion.index)):\n",
        "        for j in range(len(df_confusion.columns)):\n",
        "            plt.text(j,i,str(df_confusion.iloc[i,j]))\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(df_confusion)\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def plot_roc_curve(fpr, tpr):\n",
        "    plt.plot(fpr, tpr, color='darkblue', label='AUC = %0.2f' %(auc))\n",
        "    plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_predictions)\n",
        "# Save tpr and fpr to plot seperate ROC curve\n",
        "np.savetxt(\"/content/drive/My Drive/Colab Notebooks/tpr.txt\",tpr)\n",
        "np.savetxt(\"/content/drive/My Drive/Colab Notebooks/fpr.txt\",fpr)\n",
        "plot_roc_curve(fpr, tpr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}